{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGUtC9gtNd7ThExSLT/CKc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolekwli/final-year-project/blob/main/notebooks/fyp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltkBHcq8SxL7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfa0HGEaIU-l"
      },
      "source": [
        "The class for the actual model with 3 different types.\n",
        "\n",
        "1. The CPC model according to Oord et al.\n",
        "2. GIM model, where the last encoding layer is trained together with the autoregressor\n",
        "3. GIM model in which the last autoregressive layer is trained independently"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQzhPRhmMm1s"
      },
      "source": [
        "class FullModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        opt,\n",
        "        kernel_sizes,\n",
        "        strides,\n",
        "        padding,\n",
        "        enc_hidden,\n",
        "        reg_hidden,\n",
        "        calc_accuracy=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Entire CPC model that can be split into smaller chunks for training\n",
        "        \"\"\"\n",
        "        super(FullModel, self).__init__()\n",
        "\n",
        "        self.opt = opt\n",
        "        self.reg_hidden = reg_hidden\n",
        "        self.enc_hidden = enc_hidden\n",
        "\n",
        "        # load model\n",
        "        self.fullmodel = nn.ModuleList([])\n",
        "\n",
        "        if self.opt.model_splits == 1:\n",
        "            # CPC model\n",
        "            self.fullmodel.append(\n",
        "                independent_module.IndependentModule(\n",
        "                    opt,\n",
        "                    enc_kernel_sizes=kernel_sizes,\n",
        "                    enc_strides=strides,\n",
        "                    enc_padding=padding,\n",
        "                    enc_hidden=enc_hidden,\n",
        "                    reg_hidden=reg_hidden,\n",
        "                    calc_accuracy=calc_accuracy,\n",
        "                )\n",
        "            )\n",
        "        elif self.opt.model_splits == 5:\n",
        "            # GIM model, where the last encoding layer is trained together with the autoregressor\n",
        "            enc_input = 1\n",
        "            last_idx = len(kernel_sizes) - 1\n",
        "\n",
        "            for i in range(last_idx):\n",
        "                self.fullmodel.append(\n",
        "                    independent_module.IndependentModule(\n",
        "                        opt,\n",
        "                        enc_input=enc_input,\n",
        "                        enc_kernel_sizes=[kernel_sizes[i]],\n",
        "                        enc_strides=[strides[i]],\n",
        "                        enc_padding=[padding[i]],\n",
        "                        enc_hidden=enc_hidden,\n",
        "                        reg_hidden=reg_hidden,\n",
        "                        use_autoregressive=self.opt.use_autoregressive,\n",
        "                        calc_accuracy=calc_accuracy,\n",
        "                    )\n",
        "                )\n",
        "                enc_input = enc_hidden\n",
        "\n",
        "            self.fullmodel.append(\n",
        "                independent_module.IndependentModule(\n",
        "                    opt,\n",
        "                    enc_input=enc_input,\n",
        "                    enc_kernel_sizes=[kernel_sizes[last_idx]],\n",
        "                    enc_strides=[strides[last_idx]],\n",
        "                    enc_padding=[padding[last_idx]],\n",
        "                    enc_hidden=enc_hidden,\n",
        "                    reg_hidden=reg_hidden,\n",
        "                    use_autoregressive=True,\n",
        "                    calc_accuracy=calc_accuracy,\n",
        "                )\n",
        "            )\n",
        "        elif (\n",
        "            self.opt.model_splits == 6\n",
        "        ):  # GIM model in which the last autoregressive layer is trained independently\n",
        "            enc_input = 1\n",
        "\n",
        "            for i in range(len(kernel_sizes)):\n",
        "                self.fullmodel.append(\n",
        "                    independent_module.IndependentModule(\n",
        "                        opt,\n",
        "                        enc_input=enc_input,\n",
        "                        enc_kernel_sizes=[kernel_sizes[i]],\n",
        "                        enc_strides=[strides[i]],\n",
        "                        enc_padding=[padding[i]],\n",
        "                        enc_hidden=enc_hidden,\n",
        "                        reg_hidden=reg_hidden,\n",
        "                        use_autoregressive=self.opt.use_autoregressive,\n",
        "                        calc_accuracy=calc_accuracy,\n",
        "                    )\n",
        "                )\n",
        "                enc_input = enc_hidden\n",
        "\n",
        "            if not self.opt.use_autoregressive:\n",
        "                # append separate autoregressive layer\n",
        "                self.fullmodel.append(\n",
        "                    independent_module.IndependentModule(\n",
        "                        opt,\n",
        "                        enc_input=enc_input,\n",
        "                        enc_hidden=enc_hidden,\n",
        "                        reg_hidden=reg_hidden,\n",
        "                        use_encoder=False,\n",
        "                        enc_kernel_sizes=None,\n",
        "                        enc_strides=None,\n",
        "                        enc_padding=None,\n",
        "                        use_autoregressive=True,\n",
        "                        calc_accuracy=calc_accuracy,\n",
        "                    )\n",
        "                )\n",
        "        else:\n",
        "            raise Exception(\"Invalid option for opt.model_splits\")\n",
        "\n",
        "    def forward(self, x, filename=None, start_idx=None, n=6):\n",
        "        model_input = x\n",
        "\n",
        "        cur_device = utils.get_device(self.opt, x)\n",
        "\n",
        "        # first dimension is used for concatenating results from different GPUs\n",
        "        loss = torch.zeros(1, len(self.fullmodel), device=cur_device)\n",
        "        accuracy = torch.zeros(1, len(self.fullmodel), device=cur_device)\n",
        "\n",
        "        if n == 6:  # train all layers at once\n",
        "            for idx, layer in enumerate(self.fullmodel):\n",
        "                loss[:, idx], accuracy[:, idx], _, z = layer(\n",
        "                    model_input, filename, start_idx\n",
        "                )\n",
        "                model_input = z.permute(0, 2, 1).detach()\n",
        "        else:\n",
        "            \"\"\"\n",
        "            forward to the layer that we want to train and only output that layer's loss\n",
        "            (all other values stay at zero initialization)\n",
        "            This does not reap the memory benefits that would be possible if we trained layers completely separately \n",
        "            (by training a layer and saving its output as the dataset to train the next layer on), but enables us \n",
        "            to test the behaviour of the model for greedy iterative training\n",
        "            \"\"\"\n",
        "            assert (\n",
        "                self.opt.model_splits == 5 or self.opt.model_splits == 6\n",
        "            ), \"Works only for GIM model training\"\n",
        "\n",
        "            for idx, layer in enumerate(self.fullmodel[: n + 1]):\n",
        "                if idx == n:\n",
        "                    loss[:, idx], accuracy[:, idx], _, _ = layer(\n",
        "                        model_input, filename, start_idx\n",
        "                    )\n",
        "                else:\n",
        "                    _, z = layer.get_latents(model_input)\n",
        "                    model_input = z.permute(0, 2, 1).detach()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward_through_n_layers(self, x, n):\n",
        "        if self.opt.model_splits == 1:\n",
        "            if n > 4:\n",
        "                model_input = x\n",
        "                for idx, layer in enumerate(self.fullmodel):\n",
        "                    c, z = layer.get_latents(model_input)\n",
        "                    model_input = z.permute(0, 2, 1).detach()\n",
        "                x = c\n",
        "            else:\n",
        "                x = self.fullmodel[0].encoder.forward_through_n_layers(\n",
        "                    x, n+1\n",
        "                )\n",
        "                x = x.permute(0, 2, 1)\n",
        "        elif self.opt.model_splits == 6 or self.opt.model_splits == 5:\n",
        "            model_input = x\n",
        "            for idx, layer in enumerate(self.fullmodel[: n + 1]):\n",
        "                c, z = layer.get_latents(model_input)\n",
        "                model_input = z.permute(0, 2, 1).detach()\n",
        "            if n < 5:\n",
        "                x = z\n",
        "            else:\n",
        "                x = c\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnHSBGPaTObG"
      },
      "source": [
        "Utilities for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve3zEr_BTPzC"
      },
      "source": [
        "def distribute_over_GPUs(opt, model, num_GPU):\n",
        "    ## distribute over GPUs\n",
        "    if opt.device.type != \"cpu\":\n",
        "        if num_GPU is None:\n",
        "            model = nn.DataParallel(model)\n",
        "            num_GPU = torch.cuda.device_count()\n",
        "            opt.batch_size_multiGPU = opt.batch_size * num_GPU\n",
        "        else:\n",
        "            assert (\n",
        "                num_GPU <= torch.cuda.device_count()\n",
        "            ), \"You cant use more GPUs than you have.\"\n",
        "            model = nn.DataParallel(model, device_ids=list(range(num_GPU)))\n",
        "            opt.batch_size_multiGPU = opt.batch_size * num_GPU\n",
        "    else:\n",
        "        model = nn.DataParallel(model)\n",
        "        opt.batch_size_multiGPU = opt.batch_size\n",
        "\n",
        "    model = model.to(opt.device)\n",
        "    print(\"Let's use\", num_GPU, \"GPUs!\")\n",
        "\n",
        "    return model, num_GPU\n",
        "\n",
        "\n",
        "def genOrthgonal(dim):\n",
        "    a = torch.zeros((dim, dim)).normal_(0, 1)\n",
        "    q, r = torch.qr(a)\n",
        "    d = torch.diag(r, 0).sign()\n",
        "    diag_size = d.size(0)\n",
        "    d_exp = d.view(1, diag_size).expand(diag_size, diag_size)\n",
        "    q.mul_(d_exp)\n",
        "    return q\n",
        "\n",
        "\n",
        "def makeDeltaOrthogonal(weights, gain):\n",
        "    rows = weights.size(0)\n",
        "    cols = weights.size(1)\n",
        "    if rows > cols:\n",
        "        print(\"In_filters should not be greater than out_filters.\")\n",
        "    weights.data.fill_(0)\n",
        "    dim = max(rows, cols)\n",
        "    q = genOrthgonal(dim)\n",
        "    mid1 = weights.size(2) // 2\n",
        "    mid2 = weights.size(3) // 2\n",
        "    with torch.no_grad():\n",
        "        weights[:, :, mid1, mid2] = q[: weights.size(0), : weights.size(1)]\n",
        "        weights.mul_(gain)\n",
        "\n",
        "\n",
        "def reload_weights(opt, model, optimizer, reload_model):\n",
        "    ## reload weights for training of the linear classifier\n",
        "    if (opt.model_type == 0) and reload_model:  # or opt.model_type == 2)\n",
        "        print(\"Loading weights from \", opt.model_path)\n",
        "\n",
        "        if opt.experiment == \"audio\":\n",
        "            model.load_state_dict(\n",
        "                torch.load(\n",
        "                    os.path.join(opt.model_path, \"model_{}.ckpt\".format(opt.model_num)),\n",
        "                    map_location=opt.device.type,\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            for idx, layer in enumerate(model.module.encoder):\n",
        "                model.module.encoder[idx].load_state_dict(\n",
        "                    torch.load(\n",
        "                        os.path.join(\n",
        "                            opt.model_path,\n",
        "                            \"model_{}_{}.ckpt\".format(idx, opt.model_num),\n",
        "                        ),\n",
        "                         map_location=opt.device.type,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    ## reload weights and optimizers for continuing training\n",
        "    elif opt.start_epoch > 0:\n",
        "        print(\"Continuing training from epoch \", opt.start_epoch)\n",
        "\n",
        "        if opt.experiment == \"audio\":\n",
        "            model.load_state_dict(\n",
        "                torch.load(\n",
        "                    os.path.join(\n",
        "                        opt.model_path, \"model_{}.ckpt\".format(opt.start_epoch)\n",
        "                    ),\n",
        "                    map_location=opt.device.type,\n",
        "                ),\n",
        "                strict=False,\n",
        "            )\n",
        "        else:\n",
        "            for idx, layer in enumerate(model.module.encoder):\n",
        "                model.module.encoder[idx].load_state_dict(\n",
        "                    torch.load(\n",
        "                        os.path.join(\n",
        "                            opt.model_path,\n",
        "                            \"model_{}_{}.ckpt\".format(idx, opt.start_epoch),\n",
        "                        ),\n",
        "                        map_location=opt.device.type,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        for i, optim in enumerate(optimizer):\n",
        "            if opt.model_splits > 3 and i > 2:\n",
        "                break\n",
        "            optim.load_state_dict(\n",
        "                torch.load(\n",
        "                    os.path.join(\n",
        "                        opt.model_path,\n",
        "                        \"optim_{}_{}.ckpt\".format(str(i), opt.start_epoch),\n",
        "                    ),\n",
        "                    map_location=opt.device.type,\n",
        "                )\n",
        "            )\n",
        "    else:\n",
        "        print(\"Randomly initialized model\")\n",
        "\n",
        "    return model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKLsRRjOIXgd"
      },
      "source": [
        "This function initialises the model with dimensions given in the Oord et al paper, although padding was not mentioned.\n",
        "\n",
        "This function also makes sure only one GPU is used for when we're doign supervised loss.\n",
        "\n",
        "The function also initialises the ADAM optimiser.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdEAtdk-KO5G"
      },
      "source": [
        "def load_model_and_optimizer(\n",
        "    opt, reload_model=False, calc_accuracy=False, num_GPU=None\n",
        "):\n",
        "\n",
        "    # original dimensions given in CPC paper (Oord et al.)\n",
        "    kernel_sizes = [10, 8, 4, 4, 4]\n",
        "    strides = [5, 4, 2, 2, 2]\n",
        "    padding = [2, 2, 2, 2, 1]\n",
        "    enc_hidden = 512\n",
        "    reg_hidden = 256\n",
        "\n",
        "    ## initialize model\n",
        "    model = full_model.FullModel(\n",
        "        opt,\n",
        "        kernel_sizes=kernel_sizes,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        enc_hidden=enc_hidden,\n",
        "        reg_hidden=reg_hidden,\n",
        "        calc_accuracy=calc_accuracy,\n",
        "    )\n",
        "\n",
        "    # run on only one GPU for supervised losses\n",
        "    if opt.loss == 2 or opt.loss == 1:\n",
        "        num_GPU = 1\n",
        "\n",
        "    model, num_GPU = model_utils.distribute_over_GPUs(opt, model, num_GPU=num_GPU)\n",
        "\n",
        "    \"\"\" initialize optimizers\n",
        "    We need to have a separate optimizer for every individually trained part of the network\n",
        "    as calling optimizer.step() would otherwise cause all parts of the network to be updated\n",
        "    even when their respective gradients are zero (due to momentum)\n",
        "    \"\"\"\n",
        "    optimizer = []\n",
        "    for idx, layer in enumerate(model.module.fullmodel):\n",
        "        if isinstance(opt.learning_rate, list):\n",
        "            cur_lr = opt.learning_rate[idx]\n",
        "        else:\n",
        "            cur_lr = opt.learning_rate\n",
        "        optimizer.append(torch.optim.Adam(layer.parameters(), lr=cur_lr))\n",
        "\n",
        "    model, optimizer = model_utils.reload_weights(opt, model, optimizer, reload_model)\n",
        "\n",
        "    model.train()\n",
        "    print(model)\n",
        "\n",
        "    return model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}